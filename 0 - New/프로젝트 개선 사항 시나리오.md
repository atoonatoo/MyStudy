
---
### ChatGPT 호출량 제한
- gpt api는 소정의 금액을 충전하여 이용자가 질문 횟수에 따라 (호출 1회 = 0.03원) 비용이 소비된다. 
- 따라서 많은 요청으로 인한 한정된 비용을 절약하고, 악의적 의도를 가진 사용자에 대한 호출 방지가 필요하다.

- version 1.1 : GPT 호출량 제한
	- 구현 방식
		- `Redis`를 활용하여 사용자별 호출 횟수를 기록한다.
		- Redis의 `INCR`, `EXPIRE` 명령을 활용해 하루 단위로 자동 초기화 되도록 구성한다.
	- 문제점
		- 호출량을 제한하여 비용은 절감할 수 있지만, UX가 감소하여 이용자는 굳이 내가 만든 홈페이지를 이용할 메리트가 없어지며, 공식 홈페이지의 GPT를 이용하는게 낫다고 판단할 가능성이 높다.
	  
- version 1.2 : 캐싱 처리
	- UX를 높이기 위하여 캐싱 처리를 추가한다.
	- 동일한 질문에 대해 반복적으로 GPT를 호출하지 않고, 이전 결과를 Redis에 캐시하여 재사용함으로써 비용을 절감하고 응답 속도를 향상시킨다.
	- 구현 방식
		- 사용자가 질문을 정규화(소문자화, 특수문자 제거 등) 하여 캐시 키를 통일
		- `gpt:cache:{질문}` 형식으로 Redis에 저장하여, 최대 72시간 TTL을 적용
		- 동일 질문이 들어올 경우 GPT 호출 없이 즉시 캐시 응답을 반환
	- 문제점
		- 정규화 한계로 캐시 HIT 누락
			- 질문 표현이 조금만 달라도 캐시 미적용이 될 수 있다.
			- 개선: 의미 기반 정규화, 템플릿 매핑, 유사도 그룹화 적용
		- 오입력, 저품질 응답도 캐시됨
			- 오타·의미 없는 질문도 저장될 수 있다.
			- 개선: 응답 길이/품질 기준으로 캐시 여부 판단
		- 캐시 키 과다 생성 및 메모리 낭비
			- 유사 질문 여러 키로 저장 → 캐시 오염
			- 개선: 키 해시 적용, NLP 기반 질문 통합
		- TTL 만료로 인기 질문 캐시 소멸
			- 자주 쓰는 질문도 TTL 초과로 삭제됨
			- 개선: 캐시 HIT 시 TTL 갱신, LRU/LFU 전략 활용
		- GPT 모델 업데이트 반영 어려움
			- 캐시된 응답이 구버전 기준일 수 있음
			- 개선: 모델 버전을 캐시 키에 포함하거나 주기적 갱신
	  
- version 1.3 : 캐싱 정밀화
---
### 대량의 API를 호출하면 무슨 일이 벌어지는지? 어떻게 모니터링 할지?

### 데이터가 많을 때 조회하는 성능 개선

### 스파이크성 트래픽을 재현

---


