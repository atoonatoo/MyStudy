---
created:
---

---
- [[CS]]
- [[Operating System]]
---
### **0. 정보 단위**
- **비트(Bit)**: 
  - 0과 1로 표현되는 가장 작은 정보 단위.
  - `n비트 = 2^n가지 정보`를 표현 가능.
- **바이트(Byte)**: 
  - 8비트를 묶은 단위.
  - 1바이트는 `2^8 = 256가지 정보`를 표현 가능.
- **단위 간 관계**:
  - 1킬로바이트(1KB) = 1,000바이트  
  - 1메가바이트(1MB) = 1,000킬로바이트  
  - 1기가바이트(1GB) = 1,000메가바이트  
  - 1테라바이트(1TB) = 1,000기가바이트  

---

### **1. 워드(Word)**
- **정의**: CPU가 한 번에 처리할 수 있는 데이터 크기를 의미.  
- CPU가 16비트를 처리 가능하면 1워드는 16비트.  
- 현대 컴퓨터에서는 32비트 또는 64비트 워드 크기가 일반적.

---

### **2. 이진법**
- **정의**: 0과 1만으로 모든 숫자를 표현하는 방법.  
- 이진법으로 표현된 수 → **이진수**  
- 십진법으로 표현된 수 → **십진수**  

##### **이진수의 음수 표현**
- **2의 보수**: 
  - 모든 비트를 뒤집어(1의 보수) 1을 더한 값.
  - 컴퓨터 내부에서는 플래그를 사용해 양수/음수를 구분.
  - `N비트`로는 `-2^N`과 `2^N`을 동시에 표현 불가능.

---

### **3. 16진법**
- **정의**: 
  - 수가 15를 초과할 때 자리 올림을 하는 방식.
  - 수학적 표기: `15(16)`
  - 코드상 표기: `0x15`

##### **변환 방법**
- **16진수 → 이진수**: 
  - 16진수 한 글자를 4비트 이진수로 변환 후 이어 붙임.
  - 예: `1A2B(16) → 0001 1010 0010 1011(2)`
- **이진수 → 16진수**:
  - 4비트씩 나눠 16진수로 변환.
  - 예: `11010101(2) → D5(16)`

---

### **4. 0과 1로 문자를 표현하는 방법**
- **문자 집합(Character Set)**:
  - 컴퓨터가 인식할 수 있는 문자의 모음.
- **인코딩**: 문자를 0과 1로 변환하는 과정.  
- **디코딩**: 0과 1로 표현된 문자 코드를 사람이 이해할 수 있는 문자로 변환.

##### **아스키(ASCII) 코드**
- 영어 알파벳, 숫자, 일부 특수문자 포함.  
- 7비트로 표현(128개 문자).  

##### **ECU-KR 인코딩**
- 한글 인코딩 방식:
  - **완성형**: 초성, 중성, 종성의 조합으로 완성된 글자에 고유 코드 부여.
  - **조합형**: 초성, 중성, 종성을 각각 비트열로 할당.
- **CP949**: ECU-KR의 확장 버전, 더 다양한 문자 표현 가능.

---

### **5. 유니코드와 UTF-8**
- **유니코드**: 전 세계 문자를 통합한 표준 문자 집합.
- **UTF-8 인코딩**:
  - 유니코드 값을 1~4바이트로 가변적으로 표현.
  - 범위별 바이트 수:
    - `0 ~ 007F(16)` → 1바이트
    - `0080 ~ 07FF(16)` → 2바이트
    - `0800 ~ FFFF(16)` → 3바이트
    - `10000 ~ 10FFFF(16)` → 4바이트
